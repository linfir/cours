% autogenerated by ytex.rs

\documentclass{scrartcl}

\usepackage[francais]{babel}
\usepackage{geometry}
\usepackage{scrpage2}
\usepackage{lastpage}
\usepackage{ragged2e}
\usepackage{multicol}
\usepackage{etoolbox}
\usepackage{xparse}
\usepackage{enumitem}
% \usepackage{csquotes}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{stmaryrd}
\usepackage{dsfont}
\usepackage{eurosym}
% \usepackage{numprint}
% \usepackage[most]{tcolorbox}
% \usepackage{tikz}
% \usepackage{tkz-tab}
\usepackage[unicode]{hyperref}
\usepackage[ocgcolorlinks]{ocgx2}

\let\ifTwoColumns\iftrue
\def\Classe{$\Psi$2019--2020}

% reproducible builds
% LuaTeX: \pdfvariable suppressoptionalinfo 1023 \relax
\pdfinfoomitdate=1
\pdftrailerid{}

\newif\ifDisplaystyle
\everymath\expandafter{\the\everymath\ifDisplaystyle\displaystyle\fi}
\newcommand\DS{\displaystyle}

\clearscrheadfoot
\pagestyle{scrheadings}
\thispagestyle{empty}
\ohead{\Classe}
\ihead{\thepage/\pageref*{LastPage}}

\setlist[itemize,1]{label=\textbullet}
\setlist[itemize,2]{label=\textbullet}

\ifTwoColumns
  \geometry{margin=1cm,top=2cm,bottom=3cm,foot=1cm}
  \setlist[enumerate]{leftmargin=*}
  \setlist[itemize]{leftmargin=*}
\else
  \geometry{margin=3cm}
\fi

\makeatletter
\let\@author=\relax
\let\@date=\relax
\renewcommand\maketitle{%
    \begin{center}%
        {\sffamily\huge\bfseries\@title}%
        \ifx\@author\relax\else\par\medskip{\itshape\Large\@author}\fi
        \ifx\@date\relax\else\par\bigskip{\large\@date}\fi
    \end{center}\bigskip
    \ifTwoColumns
        \par\begin{multicols*}{2}%
        \AtEndDocument{\end{multicols*}}%
        \setlength{\columnsep}{5mm}
    \fi
}
\makeatother

\newcounter{ParaNum}
\NewDocumentCommand\Para{smo}{%
  \IfBooleanF{#1}{\refstepcounter{ParaNum}}%
  \paragraph{\IfBooleanF{#1}{{\tiny\arabic{ParaNum}~}}#2\IfNoValueF{#3}{ (#3)}}}

\newcommand\I{i}
\newcommand\mi{i}
\def\me{e}

\def\do#1{\expandafter\undef\csname #1\endcsname}
\docsvlist{Ker,sec,csc,cot,sinh,cosh,tanh,coth,th}
\undef\do

\DeclareMathOperator\ch{ch}
\DeclareMathOperator\sh{sh}
\DeclareMathOperator\th{th}
\DeclareMathOperator\coth{coth}
\DeclareMathOperator\cotan{cotan}
\DeclareMathOperator\argch{argch}
\DeclareMathOperator\argsh{argsh}
\DeclareMathOperator\argth{argth}

\let\epsilon=\varepsilon
\let\phi=\varphi
\let\leq=\leqslant
\let\geq=\geqslant
\let\subsetneq=\varsubsetneq
\let\emptyset=\varnothing

\newcommand{\+}{,\;}

\undef\C
\newcommand\ninf{{n\infty}}
\newcommand\N{\mathbb{N}}
\newcommand\Z{\mathbb{Z}}
\newcommand\Q{\mathbb{Q}}
\newcommand\R{\mathbb{R}}
\newcommand\C{\mathbb{C}}
\newcommand\K{\mathbb{K}}
\newcommand\Ns{\N^*}
\newcommand\Zs{\Z^*}
\newcommand\Qs{\Q^*}
\newcommand\Rs{\R^*}
\newcommand\Cs{\C^*}
\newcommand\Ks{\K^*}
\newcommand\Rp{\R^+}
\newcommand\Rps{\R^+_*}
\newcommand\Rms{\R^-_*}
\newcommand{\Rpinf}{\Rp\cup\Acco{+\infty}}

\undef\B
\newcommand\B{\mathscr{B}}

\undef\P
\DeclareMathOperator\P{\mathbb{P}}
\DeclareMathOperator\E{\mathbb{E}}
\DeclareMathOperator\Var{\mathbb{V}}

\DeclareMathOperator*\PetitO{o}
\DeclareMathOperator*\GrandO{O}
\DeclareMathOperator*\Sim{\sim}
\DeclareMathOperator\Tr{tr}
\DeclareMathOperator\Ima{Im}
\DeclareMathOperator\Ker{Ker}
\DeclareMathOperator\Sp{Sp}
\DeclareMathOperator\Diag{diag}
\DeclareMathOperator\Rang{rang}
\DeclareMathOperator*\Coords{Coords}
\DeclareMathOperator*\Mat{Mat}
\DeclareMathOperator\Pass{Pass}
\DeclareMathOperator\Com{Com}
\DeclareMathOperator\Card{Card}
\DeclareMathOperator\Racines{Racines}
\DeclareMathOperator\Vect{Vect}
\DeclareMathOperator\Id{Id}

\newcommand\DerPart[2]{\frac{\partial #1}{\partial #2}}

\def\T#1{{#1}^T}

\def\pa#1{({#1})}
\def\Pa#1{\left({#1}\right)}
\def\bigPa#1{\bigl({#1}\bigr)}
\def\BigPa#1{\Bigl({#1}\Bigr)}
\def\biggPa#1{\biggl({#1}\biggr)}
\def\BiggPa#1{\Biggl({#1}\Biggr)}

\def\pafrac#1#2{\pa{\frac{#1}{#2}}}
\def\Pafrac#1#2{\Pa{\frac{#1}{#2}}}
\def\bigPafrac#1#2{\bigPa{\frac{#1}{#2}}}
\def\BigPafrac#1#2{\BigPa{\frac{#1}{#2}}}
\def\biggPafrac#1#2{\biggPa{\frac{#1}{#2}}}
\def\BiggPafrac#1#2{\BiggPa{\frac{#1}{#2}}}

\def\cro#1{[{#1}]}
\def\Cro#1{\left[{#1}\right]}
\def\bigCro#1{\bigl[{#1}\bigr]}
\def\BigCro#1{\Bigl[{#1}\Bigr]}
\def\biggCro#1{\biggl[{#1}\biggr]}
\def\BiggCro#1{\Biggl[{#1}\Biggr]}

\def\abs#1{\mathopen|{#1}\mathclose|}
\def\Abs#1{\left|{#1}\right|}
\def\bigAbs#1{\bigl|{#1}\bigr|}
\def\BigAbs#1{\Bigl|{#1}\Bigr|}
\def\biggAbs#1{\biggl|{#1}\biggr|}
\def\BiggAbs#1{\Biggl|{#1}\Biggr|}

\def\acco#1{\{{#1}\}}
\def\Acco#1{\left\{{#1}\right\}}
\def\bigAcco#1{\bigl\{{#1}\bigr\}}
\def\BigAcco#1{\Bigl\{{#1}\Bigr\}}
\def\biggAcco#1{\biggl\{{#1}\biggr\}}
\def\BiggAcco#1{\Biggl\{{#1}\Biggr\}}

\def\ccro#1{\llbracket{#1}\rrbracket}
\def\Dcro#1{\llbracket{#1}\rrbracket}

\def\floor#1{\lfloor#1\rfloor}
\def\Floor#1{\left\lfloor{#1}\right\rfloor}

\def\sEnsemble#1#2{\mathopen\{#1\mid#2\mathclose\}}
\def\bigEnsemble#1#2{\bigl\{#1\bigm|#2\bigr\}}
\def\BigEnsemble#1#2{\Bigl\{#1\Bigm|#2\Bigr\}}
\def\biggEnsemble#1#2{\biggl\{#1\biggm|#2\biggr\}}
\def\BiggEnsemble#1#2{\Biggl\{#1\Biggm|#2\Biggr\}}
\let\Ensemble=\bigEnsemble

\newcommand\IntO[1]{\left]#1\right[}
\newcommand\IntF[1]{\left[#1\right]}
\newcommand\IntOF[1]{\left]#1\right]}
\newcommand\IntFO[1]{\left[#1\right[}

\newcommand\intO[1]{\mathopen]#1\mathclose[}
\newcommand\intF[1]{\mathopen[#1\mathclose]}
\newcommand\intOF[1]{\mathopen]#1\mathclose]}
\newcommand\intFO[1]{\mathopen[#1\mathclose[}

\newcommand\Fn[3]{#1\colon#2\to#3}
\newcommand\CC[1]{\mathscr{C}^{#1}}
\newcommand\D{\mathop{}\!\mathrm{d}}

\newcommand\longto{\longrightarrow}

\undef\M
\newcommand\M[3]{\mathrm{#1}_{#2}\pa{#3}}
\newcommand\MnR{\M{M}{n}{\R}}
\newcommand\MnC{\M{M}{n}{\C}}
\newcommand\MnK{\M{M}{n}{\K}}
\newcommand\GLnR{\M{GL}{n}{\R}}
\newcommand\GLnC{\M{GL}{n}{\C}}
\newcommand\GLnK{\M{GL}{n}{\K}}
\newcommand\DnR{\M{D}{n}{\R}}
\newcommand\DnC{\M{D}{n}{\C}}
\newcommand\DnK{\M{D}{n}{\K}}
\newcommand\SnR{\M{S}{n}{\R}}
\newcommand\AnR{\M{A}{n}{\R}}
\newcommand\OnR{\M{O}{n}{\R}}
\newcommand\SnRp{\mathrm{S}_n^+(\R)}
\newcommand\SnRpp{\mathrm{S}_n^{++}(\R)}

\newcommand\LE{\mathscr{L}(E)}
\newcommand\GLE{\mathscr{GL}(E)}
\newcommand\SE{\mathscr{S}(E)}
\renewcommand\OE{\mathscr{O}(E)}

\newcommand\ImplD{$\Cro\Rightarrow$}
\newcommand\ImplR{$\Cro\Leftarrow$}
\newcommand\InclD{$\Cro\subset$}
\newcommand\InclR{$\Cro\supset$}
\newcommand\notInclD{$\Cro{\not\subset}$}
\newcommand\notInclR{$\Cro{\not\supset}$}

\newcommand\To[1]{\xrightarrow[#1]{}}
\newcommand\Toninf{\To{\ninf}}

\newcommand\Norm[1]{\|#1\|}
\newcommand\Norme{{\Norm{\cdot}}}

\newcommand\Int[1]{\mathring{#1}}
\newcommand\Adh[1]{\overline{#1}}

\newcommand\Uplet[2]{{#1},\dots,{#2}}
\newcommand\nUplet[3]{(\Uplet{{#1}_{#2}}{{#1}_{#3}})}

\newcommand\Fonction[5]{{#1}\left|\begin{aligned}{#2}&\;\longto\;{#3}\\{#4}&\;\longmapsto\;{#5}\end{aligned}\right.}

\DeclareMathOperator\orth{\bot}
\newcommand\Orth[1]{{#1}^\bot}
\newcommand\PS[2]{\langle#1,#2\rangle}

\newcommand{\Tribu}{\mathscr{T}}
\newcommand{\Part}{\mathcal{P}}
\newcommand{\Pro}{\bigPa{\Omega,\Tribu}}
\newcommand{\Prob}{\bigPa{\Omega,\Tribu,\P}}

\newcommand\DEMO{$\spadesuit$}
\newcommand\DUR{$\spadesuit$}

\newenvironment{psmallmatrix}{\left(\begin{smallmatrix}}{\end{smallmatrix}\right)}

% -----------------------------------------------------------------------------

\usepackage{dsfont}

\newcommand{\SEnsemble}[2]{\{ #1 \mid #2 \}}
\newcommand{\LProb}{\LL1\Prob}
\newcommand{\LL}[1]{\mathcal{L}^{#1}}
\newcommand{\Cov}{\mathop{\mathrm{Cov}}}

\begin{document}
\title{Variables al\'eatoires discr\`etes}
\maketitle

\Para{Notations}

Dans tout le chapitre
\begin{itemize}
\item
  $\Pro$ d\'esigne un espace probabilisable;
\item
  $\Prob$ d\'esigne un espace probabilis\'e;
\item
  Toutes les variables al\'eatoires consid\'er\'ees seront des variables al\'eatoires \emph{discr\`etes}.
\end{itemize}

% -----------------------------------------------------------------------------
\section{G\'en\'eralit\'es}

\subsection{Variables al\'eatoires}

\Para{D\'efinition}

Une \emph{variable al\'eatoire (discr\`ete)} $X$ sur $\Pro$ est une fonction $\Fn{X}{\Omega}{E}$ telle que
\begin{itemize}
\item
  l'image $X(\Omega)$ de $X$ est au plus d\'enombrable;
\item
  pour tout $x\in X(\Omega)$, l'ensemble $X^{-1}\bigl(\{ x \}\bigr)$ est un \'el\'ement de la tribu $\Tribu$.
\end{itemize}

\Para{Lemme}

Dans ces conditions, pour toute partie $A$ de $E$, on a $X^{-1}(A)\in\Tribu$.
On note \og$X\in A$\fg{} l'\'ev\'enement $X^{-1}(A)$.

\Para{D\'efinition.}

Soit $X$ une variable al\'eatoire sur $\Pro$.
On dit que \emph{$X$ est \`a valeurs dans $F$} si et seulement si $X(\Omega)\subset F$.

\subsection{Loi d'une variable al\'eatoire}

\Para{D\'efinition}

Soit $X$ une variable al\'eatoire sur $\Prob$ \`a valeurs dans $E$.
La \emph{loi} de $X$ est l'application
\[ \Fonction{\P_X}{\Part(E)}{[0,1]}{A}{ \P(X\in A) = \P\bigl(X^{-1}(A)\bigr) } \]
Il s'agit d'une probabilit\'e sur l'espace probabilisable $\bigl(E,\Part(E)\bigr)$.

\Para{Proposition}

Soit $X$ une variable al\'eatoire sur $\Prob$ \`a valeurs dans $E$.
On suppose $E = \SEnsemble{x_n}{n\in \N}$ o\`u les $x_n$ sont deux \`a deux distincts.
Alors la suite $p_n = \P(X = x_n)$ caract\'erise la loi de la suite $\P_X$.
Plus pr\'ecis\'ement, pour tout $A\subset E$ on a
\[ \P_X(A) = \sum_{\substack{n\in \N\\ x_n\in A}} p_n \]

\Para{Th\'eor\`eme d'existence}

Soit $X$ une variable al\'eatoire sur $\Pro$ telle que
$X(\Omega) = \SEnsemble{x_n}{n\in \N}$ o\`u les $x_n$ sont deux \`a deux distincts.
Soit $(p_n)_{n\in \N}$ une suite de r\'eels positifs telle que la s\'erie $\sum_n p_n$ converge et $\sum_{n\geq0} p_n = 1$.
Alors il existe une probabilit\'e $\P$ sur $\Pro$ telle que
\[ \forall n\in \N\+\P(X=x_n) = p_n. \]

\Para{Remarque}

Il n'y a pas unicit\'e en g\'en\'eral.

\subsection{Fonction de r\'epartition d'une variable al\'eatoire r\'eelle}

\Para{D\'efinition}

Soit $X$ une variable al\'eatoire r\'eelle sur $\Prob$.
La \emph{fonction de r\'epartition de $X$} est la fonction $\Fn{F_X}{\R}{\R}$ d\'efinie par
\[ F_X(x) = \P(X\leq x) = \P_X(\intOF{-\infty,x}). \]

\Para{Propri\'et\'es}

Soit $X$ une variable al\'eatoire r\'eelle sur $\Prob$
et $F_X$ sa fonction de r\'epartition.
Alors, pour $(a,b) \in \R^2$, on a
\begin{enumerate}
\item
  $F_X$ est croissante sur $\R$;
\item
  $\DS \lim_{-\infty} F_X = 0$;
\item
  $\DS \lim_{+\infty} F_X = 1$;
\item
  Si $a < b$, alors $\DS \P( a < X \leq b ) = F_X(b) - F_X(a)$;
\item
  $F_X$ est \emph{continue \`a droite}, c.-\`a-d.
  $\DS\lim_{x\to a^+} F_X(x) = F_X(a)$;
\item
  $\DS \lim_{x \to a^-} F_X(x) = \P(X < a)$;
\item
  $\DS \P(X = a) = F_X(a) - \lim_{x \to a^-} F_X(x)$.
\end{enumerate}

\Para{Th\'eor\`eme}

Soit $X$ et $Y$ deux variables al\'eatoires r\'eelles sur $\Prob$.
Alors $X$ et $Y$ ont la m\^eme fonction de r\'epartition si et seulement si $X$ et $Y$ ont m\^eme loi.
\[ F_X = F_Y \iff \P_X = \P_Y. \]

\subsection{Fonction d'une ou de plusieurs variables al\'eatoires}

\Para{Lemme}

Soit $\Prob$ un espace probabilis\'e, $\Fn{X}{\Omega}{E}$ et $\Fn{Y}{\Omega}{F}$ deux variables al\'eatoires.
On pose $Z = (X,Y)$, c.-\`a-d.
\[ \Fonction{Z}{\Omega}{E\times F}{\omega}{\bigl( X(\omega), Y(\omega) \bigr)} \]
Alors $Z$ est une variable al\'eatoire \`a valeurs dans $E\times F$.

Cela se g\'en\'eralise imm\'ediatement au cas de $n$ variables al\'eatoires.

\Para{Proposition-D\'efinition}

Soit $X$ une variable al\'eatoire sur $\Pro$ \`a valeurs dans $E$
et $\Fn{f}{E}{F}$ une application quelconque.
On note $f(X)$ la fonction $f\circ X$; il s'agit d'une variable al\'eatoire.

\Para{Corollaire}

Soit $\Prob$ un espace probabilis\'e.
On consid\`ere $n$ variables al\'eatoires $\Fn{X_i}{\Omega}{E_i}$ pour $i\in\Dcro{1,n}$.
Soit $\Fn{f}{\prod_{i=1}^n E_i}{F}$ une application quelconque.
Alors $Y = f(X_1,X_2,\dots,X_n)$ est une variable al\'eatoire.

% -----------------------------------------------------------------------------
\section{Esp\'erance}

\subsection{Variables al\'eatoires d'esp\'erances finies}

\Para{D\'efinition}

Soit $X$ une variable al\'eatoire r\'eelle sur $\Prob$.
On suppose que $X(\Omega)\subset\SEnsemble{x_n}{n\in \N}\subset \R$ o\`u les $x_n$ sont deux \`a deux distincts.
On dit que $X$ est \emph{d'esp\'erance finie} si et seulement si la s\'erie $\sum_n x_n \, \P(X = x_n)$ converge absolument.
Quand c'est le cas, appelle esp\'erance de $X$ le r\'eel
\[ \E(X) = \sum_{n=0}^{+\infty} x_n \, \P(X=x_n) \]

On admet que cette d\'efinition ne d\'epend pas du choix de la suite $(x_n)_{n\in \N}$.

\Para{Formule de transfert}

Soit $E = \SEnsemble{x_n}{n\in \N}$ o\`u les $x_n$ sont deux \`a deux distincts.
Soit $X$ une variable al\'eatoire \`a valeurs dans $E$
et $\Fn{f}{E}{\R}$ une application quelconque.
Alors la variable al\'eatoire $f(X)$ est d'esp\'erance finie
si et seulement si la s\'erie $\sum_n f(x_n) \, \P(X = x_n)$ converge absolument.
Dans ce cas, on a
\[ \E\bigl( f(X) \bigr) = \sum_{n=0}^{+\infty} f(x_n) \, \P(X=x_n). \]

\Para{Proposition}[variables al\'eatoires \'egales p.s.]

Soit $X$ et $Y$ deux variables al\'eatoires \`a valeurs dans $E$.
On suppose que $X = Y$ presque s\^urement, c.-\`a-d. que $\P(X=Y) = 1$.
\begin{enumerate}
\item
  Dans le cas $E = \R$,
  $X$ est d'esp\'erance finie si et seulement si $Y$ l'est.
  Dans ce cas, on a $\E(X) = \E(Y)$.
\item
  Plus g\'en\'eralement, si $f$ est une fonction $E \to \R$,
  $f(X)$ est d'esp\'erance finie si et seulement si $f(Y)$ l'est.
  Dans ce cas, on a $\E(f(X)) = \E(f(Y))$.
\end{enumerate}

\Para{Proposition}

Soit $X$ et $Y$ deux variables al\'eatoires r\'eelles sur $\Prob$.
\begin{enumerate}
\item
  $X$ est d'esp\'erance finie si et seulement si $\Abs{X}$ est \'egalement d'esp\'erance finie.
\item
  Si $\Abs{X}\leq Y$ presque s\^urement, c.-\`a-d. si $\P\bigl(\Abs{X}\leq Y\bigr) = 1$,
  et si $Y$ est d'esp\'erance finie, alors $X$ est \'egalement d'esp\'erance finie.
\item
  En particulier, si $X$ est born\'ee, alors $X$ est d'esp\'erance finie.
\item
  Si $X$ et $Y$ sont d'esp\'erances finies, et $(\lambda,\mu)\in \R^2$,
  alors $\lambda X+\mu Y$ est \'egalement d'esp\'erance finie.
\end{enumerate}

\Para{Proposition}

Soit $X$ et $Y$ deux variables al\'eatoires r\'eelles
d'esp\'erances finies sur $\Prob$.
On a
\begin{enumerate}
\item
  Si $\lambda$ et $\mu$ sont des r\'eels, alors $\E(\lambda X +\mu Y) = \lambda \E(X) + \mu \E(Y)$.
\item
  Si $X\geq0$ presque s\^urement, alors $\E(X)\geq0$.
\item
  Si $X\leq Y$ presque s\^urement, alors $\E(X)\leq \E(Y)$.
\item
  Si $X = a$ presque s\^urement, alors $\E(X) = a$.
\item
  Si $\E\bigl( \Abs{X} \bigr) = 0$, alors $X = 0$ presque s\^urement.
\item
  Si $A\subset \R$, alors $\P(X\in A) = \E\bigl( \mathds{1}_A(X) \bigr)$.
\item
  $\bigl| \E(X) \bigr| \leq \E\bigl( |X| \bigr)$.
\end{enumerate}

\Para{D\'efinition}

Une variable al\'eatoire r\'eelle $X$ d'esp\'erance finie est dite \emph{centr\'ee} si et seulement si $\E(X) = 0$.

\Para{Proposition}[in\'egalit\'e de Markov]

Soit $\Prob$ un espace probabilis\'e et $X$ une variable al\'eatoire d'esp\'erance finie.
Si $X$ est \`a valeurs dans $\R^+$ (presque s\^urement) et si $a > 0$, alors
\[ \P(X\geq a) \leq{} \frac{\E(X)}{a}. \]

\subsection{Moments d'une variable al\'eatoire r\'eelle}

\Para{D\'efinition}

Soit $X$ une variable al\'eatoire r\'eelle sur $\Prob$.
Soit $p\in \N$.
On dit que \emph{$X$ admet un moment d'ordre $p$} si et seulement si la variable al\'eatoire $X^p$ est d'esp\'erance finie,
et on appelle \emph{moment d'ordre $p$ de $X$} le r\'eel $\E(X^p)$.

On note $\LL p\Prob$ l'ensemble des variables al\'eatoires r\'eelles sur $\Prob$ qui admettent un moment d'ordre $p$.
On notera $\LL p$ au lieu de $\LL p\Prob$ lorsque le contexte le permettra.

\Para{Proposition}

Pour tout $p\in \N$, $\LL p$ est un espace vectoriel.
De plus,
\begin{itemize}
\item
  $\LL0$ est l'ensemble des variables al\'eatoires r\'eelles sur $\Pro$;
\item
  $\LL1$ est l'ensemble des variables al\'eatoires r\'eelles sur $\Prob$ d'esp\'erance finie.
\end{itemize}

\Para{Proposition}

Soit $(p,q)\in \N^2$ tels que $p\leq q$.
Alors $\LL q \subset\LL p$.

\Para{Corollaire}

Soit $X$ une variable al\'eatoire r\'eelle. Si $X$ admet un moment d'ordre 2,
alors $X$ est d'esp\'erance finie.

\Para{Proposition}

Si $X$ et $Y$ sont deux variables al\'eatoires de $\LL2$,
alors $XY\in\LL1$.
De plus, on a l'in\'egalit\'e de Cauchy-Schwarz,
\[ \E(XY)^2 \leq \E(X^2) \,\E(Y^2). \]

\Para{D\'efinition}

Soit $X$ une variable al\'eatoire r\'eelle d'esp\'erance finie.
Notons $Y = \bigl(X -\E(X)\bigr)^2$.
Si $Y$ est d'esp\'erance finie,
on dit que $X$ est \emph{de variance finie},
et on appelle \emph{variance de $X$} le r\'eel $\E(Y)$.

\Para{Proposition}

Une variable al\'eatoire r\'eelle est de variance finie si et seulement si elle est dans $\LL2$.

\Para{Proposition}

Soit $X\in\LL2$.
\begin{enumerate}
\item
  \emph{Formule de K\"onig-Huygens}: $\Var(X) = \E(X^2) -\E(X)^2$.
\item
  Si $a\in \R$, alors $\Var(aX) = a^2 \Var(X)$ et $\Var(X+a) = \Var(X)$.
\item
  On a $\Var(X) = 0$ si et seulement s'il existe $a\in \R$ tel que $X = a$ presque s\^urement.
  Dans ce cas, $a = \E(X)$.
\end{enumerate}

\Para{In\'egalit\'e de Bienaym\'e-Tchebychev}

Soit $X$ une variable al\'eatoire r\'eelle de variance finie $\sigma^2$ et d'esp\'erance $\mu$.
Pour tout $\alpha>0$, on a
\[ \P{} \bigPa{ \Abs{X-\mu} \geq{} \alpha{} } \leq{} \frac{\sigma^2}{\alpha^2} \]

\subsection{Fonctions g\'en\'eratrices}

On s'int\'eresse ici au cas des variables al\'eatoires \`a valeurs dans $\N$.

\Para{D\'efinition}

Soit $X$ une variable al\'eatoire r\'eelle \`a valeurs dans $\N$.
On appelle \emph{fonction g\'en\'eratrice de $X$} la fonction $G_X$ d\'efinie par
$G_X(t) = \E\bigl(t^X\bigr)$.

\Para{Proposition}

Avec les m\^emes notations, posons $p_n = \P(X=n)$. On a
\[ G_X(t) = \sum_{n=0}^{+\infty} p_n t^n. \]
$G_X$ est la somme d'une s\'erie enti\`ere.
De plus,
\begin{itemize}
\item
  son rayon de convergence $R_X$ v\'erifie $R_X\geq1$;
\item
  elle converge absolument sur $\intF{-1,1}$ (au moins);
\item
  elle est continue sur $\intF{-1,1}$;
\item
  elle est de classe $\CC\infty$ sur $\intO{-R_X,R_X}$, et en particulier sur $\intO{-1,1}$.
\end{itemize}

\Para{Proposition}

Soit $X$ et $Y$ deux variables al\'eatoires sur $\Prob$ \`a valeurs dans $\N$.
Alors $X$ et $Y$ ont la m\^eme f si et seulement si $X$ et $Y$ ont la m\^eme loi.
\[ G_X = G_Y \iff \P_X = \P_Y. \]

\Para{Th\'eor\`eme}

Soit $X$ une variable al\'eatoire sur $\Prob$ \`a valeurs dans $\N$.
\begin{enumerate}
\item
  $X$ est d'esp\'erance finie si et seulement si $G_X$ est d\'erivable en $1$.
  Dans ce cas, $G_X'(1) = \E(X)$.
\item
  $X$ est de variance finie si et seulement si $G_X$ est deux fois d\'erivable en $1$.
  Dans ce cas, on a $G_X''(1) =\E\bigl[ X(X-1) \bigr]$,
  de sorte que $\Var(X) = G_X''(1) + G_X'(1) - G_X'(1)^2$.
  Il faut savoir retrouver cette derni\`ere formule.
\end{enumerate}

\Para{Remarque}

Si $X$ n'est pas \`a valeurs dans $\N$, la fonction g\'en\'eratrice $G_X$ n'est pas d\'efinie.
On travaille g\'en\'eralement avec la \emph{fonction caract\'eristique}
$\varphi_X(t) = \E\bigl(\me^{\I t X}\bigr)$, qui est toujours d\'efinie sur $\R$.

% -----------------------------------------------------------------------------
\section{Couples et suites de variables al\'eatoires}

\subsection{Couples de variables al\'eatoires}

\Para{D\'efinition}

Soit $\Prob$ un espace probabilis\'e,
$\Fn{X}{\Omega}{E}$ et $\Fn{Y}{\Omega}{F}$ deux variables al\'eatoires.
Notons $Z$ la variable al\'eatoire $Z=(X,Y)$.
La loi de $Z$ s'appelle la \emph{loi conjointe} du couple $(X,Y)$,
et les lois de $X$ et de $Y$ s'appellent les \emph{lois marginales}.
Plus explicitement, notons $E = \SEnsemble{x_i}{i\in \N}$ et $F = \SEnsemble{y_j}{j\in \N}$.
\begin{itemize}
\item
  La loi conjointe du couple $(X,Y)$ est caract\'eris\'ee par
  la famille $(p_{i,j})_{(i,j)\in \N^2}$ o\`u $\forall(i,j)\in \N^2$,
  \[ p_{i,j} = \P\bigl(Z = (x_i,y_j)\bigr) = \P( X = x_i, Y = y_j ) \]
\item
  La loi marginale de $X$ est caract\'eris\'ee par
  la famille $(q_i)_{i\in \N}$ o\`u $\forall i\in \N$,
  \[ q_i = \P(X = x_i) \]
\item
  La loi marginale de $Y$ est caract\'eris\'ee par
  la famille $(r_j)_{j\in \N}$ o\`u $\forall j\in \N$,
  \[ r_j = \P(Y = y_j) \]
\end{itemize}

\Para{Proposition}

Connaissant la loi conjointe d'un couple de variables al\'eatoires,
on peut d\'eterminer les lois marginales.
En effet, avec les m\^emes notations, on a
\[ \forall i\in \N\+ q_i = \sum_{j\in \N} p_{i,j} \quad \text{et}\quad \forall j\in \N\+ r_j = \sum_{i\in \N} p_{i,j}. \]

Attention toutefois, il n'est pas possible de d\'eterminer la loi conjointe \`a
partir des lois marginales, sauf si l'on sait que $X$ et $Y$ sont ind\'ependantes (voir ci-apr\`es).

\subsection{Ind\'ependance}

\Para{D\'efinition}

Soit $\Fn{X}{\Omega}{E}$ et $\Fn{Y}{\Omega}{F}$ deux variables al\'eatoires sur $\Prob$.
On dit que $X$ et $Y$ sont \emph{ind\'ependantes} si et seulement si $\forall(x,y)\in E\times F$,
\[ \P(X = x, Y = y) = \P(X = x) \, \P(Y = y). \]

\Para{Proposition}

Soit $\Fn{X}{\Omega}{E}$ et $\Fn{Y}{\Omega}{F}$ deux variables al\'eatoires ind\'ependantes sur $\Prob$.
Si $A\subset E$ et $B\subset F$, alors \[ \P(X\in A, Y\in B) = \P(X\in A) \, \P(Y\in B). \]

\Para{Proposition}

Soit $\Prob$ un espace probabilis\'e, $\Fn{X}{\Omega}{E}$ et $\Fn{Y}{\Omega}{F}$ deux variables al\'eatoire.
Soit $\Fn{f}{E}{E'}$ et $\Fn{g}{F}{F'}$ deux fonctions quelconques.
Si $X$ et $Y$ sont ind\'ependantes, alors les variables al\'eatoires $f(X)$ et $g(Y)$ sont \'egalement ind\'ependantes.

\Para{Proposition}

Soit $X\in\LL1$ et $Y\in\LL1$.
Si $X$ et $Y$ sont ind\'ependantes, alors $XY\in\LL1$
et $\E(XY) = \E(X) \,\E(Y)$.

\Para{Corollaire}

Soit $X$ et $Y$ deux variables al\'eatoires \`a valeurs dans $E$ et $F$ respectivement.
Soit $\Fn f E\R$ et $\Fn gF\R$ deux fonctions quelconques.
Si $X$ et $Y$ sont ind\'ependantes et si $f(X)$ et $g(Y)$ sont d'esp\'erances finies,
alors $f(X)\, g(Y)$ est \'egalement d'esp\'erance finie et
\[ \E\bigPa{f(X)\,g(Y)} = \E\bigPa{f(X)} \, \E\bigPa{g(Y)}. \]

\Para{Proposition}[fonction g\'en\'eratrice de la somme de deux variables al\'eatoires ind\'ependantes]

Soit $\Prob$ un espace probabilis\'e,
$X$ et $Y$ deux variables al\'eatoires \`a valeurs dans $\N$.
Notons $G_X$ (respectivement $G_Y$) la fonction g\'en\'eratrice de $X$ (resp. $Y$),
qui est absolument convergente sur $\mathcal{D}_X$ (resp. $\mathcal{D}_Y$)
et de rayon de convergence $R_X$ (resp. $R_Y$).
Si $X$ et $Y$ sont ind\'ependantes, alors
\begin{itemize}
\item
  $\mathcal{D}_{X+Y} \supset\mathcal{D}_X \cap\mathcal{D}_Y$,
\item
  $R_{X+Y} \geq\min(R_X, R_Y)$,
\item
  $\forall t\in\mathcal{D}_X \cap\mathcal{D}_Y$, on a $G_{X+Y} (t) = G_X(t) G_Y(t)$.
\end{itemize}

\Para{Exemple}

Utiliser ce r\'esultat pour montrer que si $X_1, \dots, X_n$ sont des variables (mutuellement) ind\'ependantes qui suivent une loi de Bernoulli $\mathcal{B}(p)$, alors $S =\sum_{k=1}^n X_k$ suit une loi binomiale $\mathcal{B}(n,p)$.

\subsection{Covariance, corr\'elation}

\Para{Proposition-D\'efinitions}

Soit $X$ et $Y$ deux variables al\'eatoires de $\LL2$ d'esp\'erances $\mu_X$ et $\mu_Y$.
Alors la variable al\'eatoire $(X - \mu_X) (Y - \mu_Y)$ est d'esp\'erance finie.
On appelle \emph{covariance de $X$ et de $Y$} le r\'eel
\[ \Cov(X,Y) = \E\bigl[ (X-\mu_X)(Y-\mu_Y) \bigr]. \]
On a \'egalement
\[ \Cov(X,Y) = \E(XY) - \mu_X \mu_Y = \E(XY) - \E(X)\E(Y). \]

\Para{Proposition}

Soit $X$, $Y$ et $Z$ trois variables al\'eatoires de $\LL2$
et $(a,b)\in \R^2$. On a
\begin{enumerate}
\item
  $\Cov(\cdot,\cdot)$ est une forme bilin\'eaire sym\'etrique positive
  (mais non d\'efinie positive) sur $\LL2$, c.-\`a-d.

  \begin{enumerate}
  \item
    $\Cov(aX+bY,Z) = a\Cov(X,Z) + b\Cov(Y,Z)$;
  \item
    $\Cov(X,aY+bZ) = a\Cov(X,Y) + b\Cov(X,Z)$;
  \item
    $\Cov(X,Y) = \Cov(Y,X)$;
  \item
    $\Cov(X,X) = \Var(X)\geq0$.
  \end{enumerate}
\item
  $\Var(aX+bY) = a^2\Var(X) + b^2\Var(Y) + 2ab \Cov(X,Y)$.
\item
  Si $X$ et $Y$ sont ind\'ependantes, $\Cov(X,Y) = 0$.
\end{enumerate}

\Para{D\'efinition}

Soit $X$ et $Y$ deux variables al\'eatoires de $\LL2$ et de variances non nulles.
On appelle \emph{coefficient de corr\'elation lin\'eaire de $X$ et de $Y$} le r\'eel
\[ \rho(X,Y) = \frac{\Cov(X,Y)}{\sqrt{\Var(X)\Var(Y)}} \]

\Para{Proposition}

Dans ces conditions,
\begin{itemize}
\item
  $\Abs{\rho(X,Y)}\leq1$
\item
  $\rho(X,Y) = \pm1$ si et seulement s'il existe $(a,b)\in \R^2$ tels que
  $Y = aX+b$ presque s\^urement.
\end{itemize}

\subsection{Suites de variables al\'eatoires}

\Para{D\'efinition}

Soit $X_1, \dots, X_n$ des variables al\'eatoires sur $\Prob$, avec
$X_i$ \`a valeurs dans $E_i$.
On dit que les variables $X_1, \dots, X_n$ sont \emph{mutuellement ind\'ependantes}
si et seulement si pour tous $(x_1, \dots, x_n) \in\prod_{i=1}^n E_i$, on a
\[ \P(X_1 = x_1, X_2 = x_2, \dots, X_n = x_n) = \prod_{i=1}^n \P(X_i = x_i). \]

\Para{D\'efinition}

Soit $(X_n)_{n\in \N}$ une suite de variables al\'eatoires.
On dit que les variables al\'eatoires $(X_n)_{n\in \N}$ sont \emph{mutuellement ind\'ependantes} si et seulement si pour tout $n\in \N$, les variables al\'eatoires $X_0, \dots, X_n$ sont mutuellement ind\'ependantes.

\Para{Remarques}
\begin{itemize}
\item
  \og Mutuellement ind\'ependantes\fg{} entra\^ine \og deux \`a deux ind\'ependantes\fg; \emph{la r\'eciproque est fausse}.
\item
  En l'absence de pr\'ecision, \og ind\'ependantes\fg{} signifie \og mutuellement ind\'ependantes\fg.
\end{itemize}

\Para{Th\'eor\`eme d'existence}

Si on se donne pour chaque $n\in \N$ une loi de probabilit\'e $\mathcal{P}_n$ sur $\R$,
alors il existe un espace probabilis\'e $\Prob$
et une suite de variables al\'eatoires r\'eelles \emph{ind\'ependantes} $(X_n)_{n\in \N}$ telles que $X_n$ suit la loi $\mathcal{P}_n$ pour tout $n$.

Plus pr\'ecis\'ement,
soit $(x_{n,k})_{(n,k)\in \N^2}$ et $(p_{n,k})_{(n,k)\in \N^2}$ deux familles de r\'eels telles que
\begin{itemize}
\item
  pour tout $n\in \N$ fix\'e, la suite $(x_{n,k})_{k\in \N}$ soit injective;
\item
  pour tous $(n,k)\in \N^2$, $p_{n,k}\geq0$;
\item
  pour tout $n\in \N$, la s\'erie $\sum_k p_{n,k}$ converge et $\sum_{k=0}^{+\infty} p_{n,k} = 1$.
\end{itemize}

Pour $n\in \N$, on pose $E_n = \SEnsemble{x_{n,k}}{k\in \N}$.
Alors il existe un espace probabilis\'e $\Prob$
et une suite de variables al\'eatoires r\'eelles $(X_n)_{n\in \N}$ sur $\Pro$ telles que:
\begin{itemize}
\item
  les variables al\'eatoires $(X_n)_{n\in \N}$ sont ind\'ependantes;
\item
  pour tout $n\in \N$, la variable al\'eatoire $X_n$ est \`a valeurs dans $E_n$
  et sa loi est d\'efinie par
  \[ \forall k\in \N\+\P(X_n = x_{n,k}) = p_{n,k}. \]
\end{itemize}

\Para{Remarque}

En g\'en\'eral, $\Omega$ n'est pas d\'enombrable et $\Tribu\neq\Part(\Omega)$.

\subsection{R\'esultats asymptotiques}

\Para{Th\'eor\`eme}[approximation de la loi binomiale par une loi de Poisson]

Soit $(p_n)_{n\in \N}$ une suite num\'erique \`a valeurs dans $[0,1]$.
Soit $(X_n)_{n\in \N}$ une suite de variables al\'eatoires r\'eelles
de loi binomiale $\mathcal{B}(n,p_n)$.
Si $np_n \Toninf\lambda\in \R$, alors
\[ \P(X_n = k) \Toninf \frac{\me^{-\lambda} \lambda^k}{k!}. \]

\Para{Th\'eor\`eme}[loi faible des grands nombres]

Soit $(X_n)_{n\geq1}$ une suite de variables al\'eatoires r\'eelles
deux \`a deux ind\'ependantes, de m\^eme loi et admettant un moment d'ordre 2.
Notons $\mu= \E(X_1)$ et
$S_n = \sum_{k=1}^n X_k$.
Alors
\[ \P\left( \left| \frac{S_n}{n} - \mu\right| \geq \epsilon\right) \Toninf 0. \]

\Para{Remarques}
\begin{itemize}
\item
  La quantit\'e $\frac{S_n}{n}$ s'appelle la \emph{moyenne empirique} de $(X_n)_{n\geq1}$.
\item
  Le r\'esultat pr\'ec\'edent reste vrai si l'on remplace l'hypoth\`ese \og admettre un moment d'ordre 2\fg{}
  par l'hypoth\`ese plus faible \og \^etre d'esp\'erance finie\fg.
\end{itemize}

% -----------------------------------------------------------------------------
\section{Lois usuelles}

\subsection{Lois finies}

\Para{Loi uniforme}

On dit que $X$ suit la loi uniforme sur l'ensemble fini $F$,
et on note $X\hookrightarrow\mathcal{U}(F)$
si $X$ est une variable al\'eatoire \`a valeurs dans $F$ telle que
\[ \forall x\in F \+ \P(X=x) = \frac{1}{\Card(F)}. \]
Par exemple, si $F = \Dcro{1,n}$, alors:
\[ \E(X) = \frac{n+1}{2}, \quad \Var(X) = \frac{n^2-1}{12}, \quad G_X(t) = \frac{t(1-t^n)}{n(1-t)}. \]

\Para{Loi de Bernoulli}

On dit que $X$ suit la loi de Bernoulli de param\`etre $p\in[0,1]$,
et on note $X\hookrightarrow\mathcal{B}(p)$
si $X$ est une variable al\'eatoire \`a valeurs dans $\{0,1\}$ telle que
\[ \P(X=0) = q, \quad \P(X=1) = p, \quad \text{o\`u } q = 1-p. \]
\[ \E(X) = p, \quad \Var(X) = pq, \quad G_X(t) = q + pt. \]

\Para{Loi binomiale}

On dit que $X$ suit la loi binomiale de param\`etres $n\in \N$ et $p\in[0,1]$,
et on note $X\hookrightarrow\mathcal{B}(n,p)$
si $X$ est une variable al\'eatoire \`a valeurs dans $\Dcro{0,n}$ telle que
\[ \forall k\in\Dcro{0,n} \+ \P(X=k) = \binom{n}{k} p^k q^{n-k},
\quad \text{o\`u } q = 1-p. \]
\[ \E(X) = np, \quad \Var(X) = npq, \quad G_X(t) = (q + pt)^n. \]

\subsection{Lois discr\`etes}

\Para{Loi g\'eom\'etrique}

On dit que $X$ suit la loi g\'eom\'etrique de param\`etre $p\in\intO{0,1}$,
et on note $X\hookrightarrow\mathcal{G}(p)$
si $X$ est une variable al\'eatoire \`a valeurs dans $\N^*$ telle que
\[ \forall k\in \N^* \+ \P(X=k) = q^{k-1} p,
\quad \text{o\`u } q = 1-p. \]
\[ \E(X) = \frac{1}{p}, \quad \Var(X) = \frac{q}{p^2}, \quad G_X(t) = \frac{pt}{1-qt}. \]

\Para{Loi de Poisson}

On dit que $X$ suit la loi de Poisson de param\`etre $\lambda\in \R^+$,
et on note $X\hookrightarrow\mathcal{P}(\lambda)$
si $X$ est une variable al\'eatoire \`a valeurs dans $\N$ telle que
\[ \forall k\in \N\+ \P(X=k) = \frac{\me^{-\lambda} \lambda^k}{k!}. \]
\[ \E(X) = \lambda, \quad \Var(X) = \lambda, \quad G_X(t) = \me^{\lambda(t-1)}. \]

% -----------------------------------------------------------------------------
\section{Exercices}

% -----------------------------------------------------------------------------
\par\pagebreak[1]\par
\paragraph{Exercice 1}%
\hfill{\tiny 3047}%
\begingroup~

D\'emontrer les formules de la partie \og lois usuelles\fg.
\endgroup

% -----------------------------------------------------------------------------
\par\pagebreak[1]\par
\paragraph{Exercice 2 (paradoxe de St Petersbourg)}%
\hfill{\tiny 9257}%
\begingroup~

Une banque vous propose de jouer au jeu suivant:
votre mise est de 100\euro.
Ensuite, vous lancez une pi\`ece \'equilibr\'ee jusqu'\`a ce que vous obteniez \og pile\fg{}
On note $X$ le nombre de lanc\'es n\'ecessaires.
Votre gain est de $G = 2^X$.
\begin{enumerate}
\item
  \begin{enumerate}
  \item
    Montrer que $X$ est \`a valeurs dans $\N^*\cup\{+\infty\}$.
  \item
    Calculer $\P(X = +\infty)$.
  \item
    Reconna\^itre la loi de $X$.
  \end{enumerate}
\item
  Calculer $\E(G)$. Devriez-vous jouer \`a ce jeu?
\item
  En fait, si $G$ est trop \'elev\'e, la banque fera faillite.
  D'apr\`es Wikipedia, le PIB mondial pour 2013 est environ $73~\mathrm{TUS\$}$ (Wikipedia).
  On note $H = \min(G,2^{50})$ le gain corrig\'e.
  D\'eterminer l'esp\'erance de $H$.
  Devriez-vous jouer \`a ce jeu?
\end{enumerate}
\endgroup

% -----------------------------------------------------------------------------
\par\pagebreak[1]\par
\paragraph{Exercice 3}%
\hfill{\tiny 0270}%
\begingroup~

Environ $5\%$ des r\'eservations a\'eriennes sur une ligne donn\'ee ne sont
pas utilis\'ees, et c'est pourquoi une compagnie vend 100 billets pour
97 places; on parle de \og surbooking\fg.
Quelle est la probabilit\'e pour que tous les passagers aient une place?
\endgroup

% -----------------------------------------------------------------------------
\par\pagebreak[1]\par
\paragraph{Exercice 4 (loi de Pascal)}%
\hfill{\tiny 1237}%
\begingroup~

Soit $(X_n)_{n\in \N^*}$ une suite de variables al\'eatoires ind\'ependantes
suivant une loi de Bernoulli $\mathcal{B}(p)$ avec $p\in\intOF{0,1}$.
Soit $k\in \N$ fix\'e.
On note $T = \min \Ensemble{n\in \N}{X_1 + \dots + X_n = k}\in \N\cup\acco{\infty}$.
\begin{enumerate}
\item
  D\'ecrire en fran\c cais ce que repr\'esente $T$.
\item
  Montrer que $T<\infty$ presque s\^urement.
  On pourra introduire $S_n = X_1 + \dots + X_n$
  et utiliser la loi des grands nombres pour montrer que
  $\P\bigPa{\frac{S_n}{n} \leq\frac{p}{2}} \to 0$ quand $n\to+\infty$.
\item
  D\'eterminer la loi de $T$.
  On dit que $T$ suit la loi de Pascal de param\`etres $k$ et $p$.
\item
  D\'eterminer l'esp\'erance, la variance de $T$
  et la fonction g\'en\'eratrice de $T$.
\item
  Soit $U = Y_1 + \dots + Y_k$ o\`u $Y_i\hookrightarrow\mathcal{G}(p)$.
  Montrer que $T$ et $U$ ont la m\^eme loi.
\end{enumerate}
\endgroup

% -----------------------------------------------------------------------------
\par\pagebreak[1]\par
\paragraph{Exercice 5}%
\hfill{\tiny 3884}%
\begingroup~

Soit $(X_{i,j})_{1\leq i,j\leq n}$ une famille de variables al\'eatoires ind\'ependantes de m\^eme loi telles que $P(X_{i,j} = 1) = \P(X_{i,j} = -1) = \frac12$.
On note $M$ la matrice dont les coefficients sont les $(X_{i,j})$.
Quelle est l'esp\'erance du d\'eterminant de $M$?
\endgroup

% -----------------------------------------------------------------------------
\par\pagebreak[1]\par
\paragraph{Exercice 6}%
\hfill{\tiny 3888}%
\begingroup~

On consid\`ere une exp\'erience al\'eatoire ayant une probabilit\'e $p$ de r\'eussir et $1-p$ d'\'echouer.
On r\'ep\`ete l'exp\'erience de fa\c con ind\'ependante jusqu'\`a obtention de $n$ succ\`es.
On note $X$ le nombre d'essais n\'ecessaires \`a l'obtention de ces $n$ succ\`es.
\begin{enumerate}
\item
  Reconna\^itre la loi de $X$ lorsque $n=1$.
\item
  D\'eterminer la loi de $X$ dans le cas g\'en\'eral.
\item
  Exprimer le d\'eveloppement en s\'erie enti\`ere de
  $(1-t)^{-n-1}$.
\item
  D\'eterminer la fonction g\'en\'eratrice de $X$
  et en d\'eduire l'esp\'erance de $X$.
\end{enumerate}
\endgroup

% -----------------------------------------------------------------------------
\par\pagebreak[1]\par
\paragraph{Exercice 7}%
\hfill{\tiny 0744}%
\begingroup~

\begin{enumerate}
\item
  Soit $X\hookrightarrow\mathcal{P}(\lambda)$.
  Montrer que l'\'ev\'enement \og$X$ est pair\fg{} est plus probable que l'\'ev\'enement \og$X$ est impair\fg.
\item
  M\^eme question avec $X\hookrightarrow\mathcal{G}(p)$.
\end{enumerate}
\endgroup

% -----------------------------------------------------------------------------
\par\pagebreak[1]\par
\paragraph{Exercice 8}%
\hfill{\tiny 3900}%
\begingroup~

Soit $X\hookrightarrow\mathcal{P}(\lambda)$ et $Y\hookrightarrow\mathcal{P}(\mu)$ deux variables al\'eatoires ind\'ependantes.
Quelle est la loi de $X+Y$?
\endgroup

% -----------------------------------------------------------------------------
\par\pagebreak[1]\par
\paragraph{\href{https://psi.miomio.fr/exo/6841.pdf}{Exercice 9}}%
\hfill{\tiny 6841}%
\begingroup~

Le jour de l'examen de fin d'ann\'ee, $n$ \'el\`eves n'ont pas assez soign\'e la pr\'esentation de leur copie.
Le correcteur, plut\^ot que de s'escrimer \`a lire d'inf\^ames brouillons, d\'ecide de noter au hasard, de mani\`ere ind\'ependante, les $n$ copies, en leur attribuant une note enti\`ere, au hasard, entre 0 et 20.
On note $X_n$ la variable al\'eatoire \'egale \`a la meilleure note du groupe.
\begin{enumerate}
\item
  Soit $k\in\Dcro{0,20}$.
  Calculer la probabilit\'e que toutes les notes soient inf\'erieures ou \'egales \`a $k$.
\item
  \begin{enumerate}
  \item
    Calculer $\P(X_n < k \mid X_n\leq k)$.
  \item
    En d\'eduire $\P( X_n = k \mid X_n\leq k)$.
  \end{enumerate}
\item
  Calculer $\P(X_n = k)$ et d\'eterminer sa limite lorsque $n \to\infty$.
  Interpr\'eter.
\end{enumerate}
\endgroup

% -----------------------------------------------------------------------------
\par\pagebreak[1]\par
\paragraph{Exercice 10 (lois sans viellissement)}%
\hfill{\tiny 4942}%
\begingroup~

Soit $X$ une variable al\'eatorie \`a valeurs dans $\N^*$,
telle que $\forall n\in \N^*$, $\P(X=n) > 0$.
On suppose de plus que pour tous $(n,k)\in \N^2$,
\[ \P(X > n+k \mid X > k) = \P(X > n). \]
Montrer que $X$ suit une loi g\'eom\'etrique dont on pr\'ecisera le param\`etre.
\endgroup

% -----------------------------------------------------------------------------
\par\pagebreak[1]\par
\paragraph{Exercice 11}%
\hfill{\tiny 2165}%
\begingroup~

Expliquer pourquoi $99,\!9\,\%$ des gens poss\`edent un nombre de jambes strictement sup\'erieur \`a la moyenne.
\endgroup

% -----------------------------------------------------------------------------
\par\pagebreak[1]\par
\paragraph{Exercice 12}%
\hfill{\tiny 0201}%
\begingroup~

Soit $X\hookrightarrow\mathcal{G}(p)$.
On pose $Y = 1/X$. Calculer $\E(Y)$.
\endgroup

% -----------------------------------------------------------------------------
\par\pagebreak[1]\par
\paragraph{Exercice 13 (\`a conna\^itre)}%
\hfill{\tiny 2198}%
\begingroup~

\def\LL#1{\mathcal{L}^{#1}}
Soit $X$ une variable al\'eatoire \`a valeurs dans $\N$.
\begin{enumerate}
\item
  \begin{enumerate}
  \item
    Montrer que que $\P(X = n) = \P(X > n-1) - \P(X > n)$ pour tout $n\in \N^*$.
  \item
    Si $X\in\LL1$, montrer que $n\P(X>n) \Toninf 0$.
  \item
    Si $X\in\LL1$, montrer que \[ \E(X) = \sum_{n\geq0} \P(X > n). \]
  \item
    Si $X\notin\LL1$, montrer que la s\'erie $\sum_n \P(X>n)$ diverge.
  \end{enumerate}
\item
  Si $X\in\LL2$, montrer de m\^eme que
  \[ \E(X^2) = \sum_{n\geq0} (2n+1) \, \P(X > n). \]
\end{enumerate}
\endgroup

% -----------------------------------------------------------------------------
\par\pagebreak[1]\par
\paragraph{Exercice 14}%
\hfill{\tiny 2480}%
\begingroup~

Soit $X\hookrightarrow\mathcal{P}(\lambda)$.
Montrer que $\P\bigl(X\leq\frac\lambda2\bigr)\leq\frac4\lambda$
et $\P\bigl(X\geq2\lambda\bigr)\leq\frac1\lambda$.
\endgroup

% -----------------------------------------------------------------------------
\par\pagebreak[1]\par
\paragraph{Exercice 15}%
\hfill{\tiny 5183}%
\begingroup~

\def\Cov{\mathop{\mathrm{Cov}}}
Soit $X$ et $Y$ deux variables al\'eatoires de lois de Bernoulli respectives $\mathcal{B}(p)$ et $\mathcal{B}(p')$.
Montrer qu'elles sont ind\'ependantes si et seulement si $\Cov(X,Y) = 0$.
\endgroup

% -----------------------------------------------------------------------------
\par\pagebreak[1]\par
\paragraph{Exercice 16}%
\hfill{\tiny 9097}%
\begingroup~

Soit $X$ une variable al\'eatoire discr\`ete \`a valeurs dans $E$
et $\Fn{f}{E}{F}$ une application.
\`A quelle condition les variables al\'eatoires $X$ et $f(X)$ sont-elles ind\'ependantes?
\endgroup

% -----------------------------------------------------------------------------
\par\pagebreak[1]\par
\paragraph{Exercice 17}%
\hfill{\tiny 1218}%
\begingroup~

Soit $X_1, \dots, X_n$ des variables al\'eatoires ind\'ependantes \`a valeurs dans $\N$.
On pose $Y = \alpha_1 X_1 + \dots + \alpha_n X_n$ o\`u $\nUplet\alpha1n \in \N^n$.
Exprimer la s\'erie g\'en\'eratrice $G_Y$ en fonction des $G_{X_i}$.
\endgroup

% -----------------------------------------------------------------------------
\par\pagebreak[1]\par
\paragraph{Exercice 18}%
\hfill{\tiny 2799}%
\begingroup~

On consid\`ere une succession (infine) de tirages \`a \og pile ou face\fg, avec une pi\`ece donnant \og pile\fg{} avce une probabilit\'e $p\in\intO{0,1}$.
On note $N$ le nombre de tirages n\'ecessaires pour avoir le premier \og pile\fg.
On effectue alors $N$ nouveaux tirages, et l'on note $X$ le nombre de \og pile\fg{} obtenus pendant ces nouveaux tirages.
Montrer que $\E(X) = 1$.
\endgroup

% -----------------------------------------------------------------------------
\par\pagebreak[1]\par
\paragraph{Exercice 19 (loi binomiale et loi de Poisson)}%
\hfill{\tiny 2723}%
\begingroup~

Soit $(X_n)$ une suite de variables al\'eatoires telles que $X_n\hookrightarrow\mathscr{B}(n,p_n)$
avec $p_n = \frac{\lambda}{n}$.
\begin{enumerate}
\item
  D\'eterminer la limite de $\E(X_n)$ et de $\Var(X_n)$.
\item
  D\'eterminer la limite de $\P(X_n = k)$ quand $n\to+\infty$.
\item
  Comment interpr\'eter ces r\'esultats?
\end{enumerate}
\endgroup

% -----------------------------------------------------------------------------
\par\pagebreak[1]\par
\paragraph{Exercice 20}%
\hfill{\tiny 3577}%
\begingroup~

Dans une grande ville, il y a en moyenne un suicide par jour.
Combien y a-t-il, en moyenne, de jours de l'ann\'ee o\`u au moins $5$ personnes
se suicident?
\endgroup

% -----------------------------------------------------------------------------
\par\pagebreak[1]\par
\paragraph{Exercice 21}%
\hfill{\tiny 0282}%
\begingroup~

Tous les soirs, au lieu de r\'eviser ses cours de pr\'epa, Kevin va rusher Stratholme
dans l'espoir de looter la monture du Baron Vaillefendre.
\begin{enumerate}
\item
  \'Etant donn\'e que WoW est un programme informatique (consid\'er\'e comme constant),
  par quelle loi mod\'eliseriez-vous cette exp\'erience?
\item
  Il semblerait que la probabilit\'e de dropper la monture soit de $0,7\%$.
  Au bout de combien de jours Kevin peut-il esp\'erer frimer sur le destrier de la mort?
  Au fait, d'o\`u peut provenir ce chiffre de $0,7\%$?
\item
  Quelle est la probabilit\'e d'arriver \`a la dropper en moins de 210 jours
  (entre le d\'ebut et la fin des cours)?
  On pourra utiliser la calculatrice, ou l'approximation par une loi de Poisson.
\item
  Question subsidiaire: que pensez-vous des chances de r\'eussite aux concours de Kevin?
\end{enumerate}
\endgroup

% -----------------------------------------------------------------------------
\par\pagebreak[1]\par
\paragraph{Exercice 22 (moindres carr\'es)}%
\hfill{\tiny 7813}%
\begingroup~

\def\LL#1{\mathcal{L}^{#1}}
\begin{enumerate}
\item
  Soit $X\in\LL2$.
  Quelle valeur de $a\in \R$ minimise-t-elle la quantit\'e
  $\E\bigl[ (X-a)^2 \bigr]$?
\item
  Soit $X$ et $Y$ deux variables al\'eatoires de $\LL2$.
  D\'eterminer $(a,b)\in \R^2$ qui minimise
  $\E\bigl[ (Y - aX-b)^2 \bigr]$.
\end{enumerate}
\endgroup

% -----------------------------------------------------------------------------
\par\pagebreak[1]\par
\paragraph{Exercice 23}%
\hfill{\tiny 9175}%
\begingroup~

Soit $X$ et $Y$ deux variables al\'eatoires \`a valeurs dans $\N$ de loi conjointe
\[ \forall(i,j)\in \N^2 \+ \P(X = i, Y = j) = \frac{a}{i!j!}. \]
\begin{enumerate}
\item
  D\'eterminer $a$.
\item
  Reconna\^itre les lois marginales de $X$ et de $Y$.
\item
  $X$ et $Y$ sont-elles ind\'ependantes?
\end{enumerate}
\endgroup

% -----------------------------------------------------------------------------
\par\pagebreak[1]\par
\paragraph{\href{https://psi.miomio.fr/exo/1635.pdf}{Exercice 24} (in\'egalit\'e de Jensen)}%
\hfill{\tiny 1635}%
\begingroup~

Soit $I$ un intervalle de $\R$, $\Fn{f}{I}{\R}$ une fonction convexe et
$X$ une variable al\'eatoire r\'eelle \`a valeurs dans $I$ d'esp\'erance finie.
L'in\'egalit\'e de Jensen affirme alors que, si $f(X)$ est d'esp\'erance finie, on a
\[ \E\bigl(f(X)\bigr)\geq f\bigl(\E(X) \bigr). \]
\begin{enumerate}
\item
  On suppose $f$ d\'erivable sur $I$.

  \begin{enumerate}
  \item
    Soit $\mu\in I$. Montrer qu'il existe $(a,b)\in \R^2$ tels que
    $\forall x\in I$, $f(x)\geq ax+b$ et $f(\mu) = a\mu+ b$.
  \item
    Montrer que $\E(X)\in I$.
  \item
    Conclure.
  \end{enumerate}
\item
  (dur) Montrer que le r\'esultat de la question 1a. reste vrai
  si l'on ne suppose plus $f$ d\'erivable. Conclure.
\end{enumerate}
\endgroup

% -----------------------------------------------------------------------------
\par\pagebreak[1]\par
\paragraph{\href{https://psi.miomio.fr/exo/8560.pdf}{Exercice 25} (identit\'e de Wald)}%
\hfill{\tiny 8560}%
\begingroup~

Soit $N$ et $(X_n)_{n\geq1}$ des variables al\'eatoires ind\'ependantes \`a valeurs dans $\N$ et d'esp\'erances finies.
On suppose que les $(X_n)_{n\geq1}$ ont toutes la m\^eme loi.
On pose \[ S = \sum_{k=1}^N X_k. \]
\begin{enumerate}
\item
  Montrer que $G_S(t) = G_N\bigl( G_{X_1}(t) \bigr)$ pour $\Abs{t}<1$.
  On admettra que l'on peut permuter les sommes.
\item
  \'Etablir l'\emph{identit\'e de Wald}:
  \[ \E(S) = \E(N) \, \E(X_1). \]
\end{enumerate}
\endgroup

\end{document}
